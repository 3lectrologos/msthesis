\chapter{Experiments} \label{ch:exp} 
In this section, we present the results of evaluating our proposed algorithms on
three real-world datasets and compare them to the state-of-the-art in sequential
level set estimation.
In more detail, the algorithms and their setup are as follows.
\begin{description}[labelindent=0pt,leftmargin=7pt,itemindent=-2pt,itemsep=0pt]
\item[\acl/\iacl:] Since the bound of \theoremref{thm:acl} is fairly conservative,
  in our experiments we used a constant value of $\beta_t^{1/2} = 3$, which is
  somewhat smaller than the values suggested by the theorem.
\item[\bacl/\ibacl:] We used $\eta_t^{1/2} = 4$ and a batch size of $B = 30$.
  Furthermore, as proposed by \citet{desautels12}, we initialized the
  algorithms with $T^{init} = 30$ sequentially chosen maximum variance samples.
\item[\str:] The state-of-the-art straddle heuristic,
  as proposed by \citet{bryan05}, with the selection rule
  ${\*x_t = \argmax_{\*x \in D} \left(1.96\sigma_{t-1}(\*x) - |\mu_{t-1}(\*x) - h|\right)}$.
\item[\istr:] For the implicit threshold setting, we have defined a
  variant of the straddle heuristic that uses at each step the
  implicitly defined threshold level with respect to the maximum of the
  inferred mean, i.e.
  ${h_t = \omega \max_{\*x\in D} \mu_{t-1}(\*x)}$.
\item[\rstr/\bstr:] We have defined two batch versions of the straddle heuristic:
  \rstr selects the $B = 30$ points with
  the largest straddle score, while \bstr follows a similar approach to \bacl
  by using the following selection rule
  \begin{align*}
  {\*x_t = \argmax_{\*x \in D}\left(1.96\sigma_{t-1}(\*x) - |\mu_{\fb[t]}(\*x) - h|\right)}.
  \end{align*}
\item[\var:]  The maximum variance rule
  $\*x_t = \argmax_{\*x \in D} \sigma_{t-1}(\*x)$.
\end{description}

\paragraph{Dataset 1: Network latency}
Our first dataset consists of round-trip time (RTT) measurements obtained
by ``pinging'' $1768$ servers spread around the world (see \figref{fig:map}).
The sample space consists of the longitude and latitude coordinates of each
server, as determined by a commercial geolocation
database\footnote{\url{http://www.maxmind.com}}.
Example applications for geographic RTT level set estimation, include
monitoring global networks and improving quality of service for applications
such as internet telephony or online games. Furthermore,
selecting samples in batches results in significant time savings, since
sending out and receiving multiple ICMP packets can be virtually performed in
parallel.

We used $200$ randomly selected measurements to fit suitable hyperparameters for
an anisotropic Mat\'{e}rn-5~\mbox{\cite{rasmussen06}} kernel by maximum
likelihood  and the remaining $1568$ for evaluation.
The threshold level we chose for the experiments was $h = 200$ ms.

%\setlength\figureheight{1.7in}\setlength\figurewidth{3.4in}
%\input{figures/map.tex}
\begin{figure}[tb]
  \centering
  \adjincludegraphics[width=4in]{figures/map}
  \caption{Round-trip times in milliseconds obtained by pinging from Zurich
           1768 servers around the world.}
  \label{fig:map}
\end{figure}

\paragraph{Datasets 2 \& 3: Environmental monitoring}
Our second and third datasets come from the domain of environmental monitoring
of inland
waters and consist of $2024$ \emph{in situ} measurements of chlorophyll
and \emph{Planktothrix rubescens}\footnote{Planktothrix rubescens is a genus of
blue-green algae that can produce toxins.}
concentration respectively, which were collected by an autonomous surface
vessel within a vertical transect plane of Lake Zurich \cite{hitz12}.
Monitoring chlorophyll and algae concentration is useful in analyzing
limnological phenomena such as algal bloom. Since the concentration levels can
vary throughout the year, in addition to having a fixed threshold
concentration, it can also be useful to be able to detect
relative ``hotspots'' of chlorophyll or algae, i.e. regions of high
concentration with respect to the current maximum. Furthermore, selecting
batches of points can be used to plan sampling paths and reduce the required
traveling distances.

In our evaluation, we used $10,000$ points sampled in a $100 \times 100$ grid
from the GP posteriors that were derived using the $2024$ original measurements
(see \figsref{fig:limno_chl} and \ref{fig:limno_bgape}).
Again, anisotropic Mat\'{e}rn-5 kernels were used and suitable
hyperparameters were fitted by maximizing the
likelihood of two different chlorophyll and algae concentration datasets from
the same lake.
As illustrated in \figsref{fig:limno_chl} and \ref{fig:limno_bgape}, we used
explicit threshold levels of $h = 1.3$ RFU (relative fluorescence units) for the
chlorophyll dataset and
$h = 7$ RFU for the algae concentration dataset. For the implicit threshold
experiments, we chose the values of $\omega$ so that the resulting implicit
levels are identical to the explicit ones, which enables us to compare the two
settings on equal ground.
%The effect of batch sampling on the required traveling distance was evaluated
%using an approximate Euclidean TSP solver to create paths connecting each
%batch of samples selected by \bacl, as described in \sectref{sect:pp}.

\paragraph{Evaluation methodology}
We assess the classification accuracy for all algorithms using the $F_1$-score,
i.e. the harmonic mean of precision and recall, by considering points in the
super- and sublevel sets as positives and negatives respectively.

Note that the execution of the algorithms is not deterministic, because
ties in the next point selection rules are resolved uniformly at random.
Additionally, in the case of the environmental monitoring datasets, sampling
from the GP posterior inherently involves randomness because of the noise
included in the GP model. For these reasons, in our evaluation we repeated
multiple executions of each algorithm. In particular, \str and its
extensions as well as \var were executed $50$ times on each dataset and the
$F_1$-score at each iteration of each execution was computed by classifying
points according to the posterior mean of the GP trained on the already
selected points, i.e.
\begin{align*}
H_t &= \{\*x \in D \mid \mu_t(\*x) \geq h\}\\
L_t &= \{\*x \in D \mid \mu_t(\*x) < h\}.
\end{align*}
On the other hand, \acl and its extensions were evaluated by repeatedly
executing each algorithm for a range of values of the accuracy parameter
$\epsilon$ and  recording the total number of samples and the $F_1$-score after
each execution's termination (in total $2000$ executions per algorithm
per dataset).
The parameter $\epsilon$ was chosen to increase
exponentially within a suitable range depending on the experiment (roughly
between $1\%$ and $20\%$ of the respective threshold level).

%\setlength\figureheight{1.3in}\setlength\figurewidth{2.1in}
%\input{figures/ev_ping_seq.tex}
%\input{figures/ev_chl_seq.tex}
%\input{figures/ev_bgape_seq.tex}
%\input{figures/ev_bgape_seq_str_sc.tex}
%\input{figures/ev_bgape_seq_acl_sc.tex}
%\input{figures/limno_bgape_str_class400.tex}
%\input{figures/limno_bgape_acl_class374.tex}
%\input{figures/limno_bgape_var_class400.tex}
\renewcommand\trimlen{2pt}
\begin{figure*}[htbp]
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ev_ping_seq}
    \vspace{-18pt}
    \caption{\textsf{[N]} Mean performance}
	  \label{fig:ping-seq}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ev_chl_seq}
    \vspace{-18pt}
    \caption{\textsf{[C]} Mean performance}
	\label{fig:chl-seq}
  \end{subfigure}

  \begin{subfigure}[b]{0.49\textwidth}
    \vspace{8pt} % space of this row from above captions
    \centering
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ev_bgape_seq}
    \vspace{-18pt}
    \caption{\textsf{[A]} Mean performance}
	  \label{fig:bgape-seq}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/limno_bgape_str_class400}
    \vspace{-18pt}
    \caption{\textsf{[A]} \str after $t=400$ iterations}
	\label{fig:bgape-str-class}
  \end{subfigure}

  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \vspace{8pt} % space of this row from above captions
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/limno_bgape_acl_class374}
    \vspace{-18pt}
    \caption{\textsf{[A]} \acl after $t=374$ iterations}
	  \label{fig:bgape-acl-class}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/limno_bgape_var_class400}
    \vspace{-18pt}
    \caption{\textsf{[A]} \var after $t=400$ iterations}
	  \label{fig:bgape-var-class}
  \end{subfigure}

  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \vspace{8pt} % space of this row from above captions
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ev_bgape_seq_str_sc}
    \vspace{-18pt}
    \caption{\textsf{[A]} \str scatter}
	  \label{fig:bgape-seq-str-sc}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ev_bgape_seq_acl_sc}
    \vspace{-18pt}
    \caption{\textsf{[A]} \acl scatter}
	  \label{fig:bgape-seq-acl-sc}
  \end{subfigure}

  \caption{Performance of explicit threshold sequential algorithms on the
           network latency \textsf{[N]},
           chlorophyll concentration \textsf{[C]},
           and algae concentration \textsf{[A]} datasets.
           \textbf{(a), (b)} \acl and \str are comparable and both clearly outperform
           \var.
           \textbf{(c)} \str performs poorly due to limited exploration.
           \textbf{(d)} An example of a \str execution getting stuck: even after $400$
           iterations it has only achieved an $F_1$-score of $0.37$.
           \textbf{(e)} An example of a \acl execution without any issues.
           \textbf{(f)} An example of a \var execution: the sample space is sampled
           rather uniformly.
           \textbf{(g), (h)} The results of (c) in more detail: about a third of \str's
           executions achieve an $F_1$-score of less than $0.4$, while \acl
           always achieves an $F_1$-score of at least $0.95$.
           }
  \label{fig:exp}
\end{figure*}

%\setlength\figureheight{1.3in}\setlength\figurewidth{2.1in}
%\input{figures/ev_ping_rule.tex}
%\input{figures/ev_chl_rule.tex}
%\input{figures/ev_bgape_rule.tex}
%\input{figures/ev_ping_rule2.tex}
%\input{figures/ev_chl_rule2.tex}
%\input{figures/ev_bgape_rule2.tex}
\renewcommand\trimlen{2pt}
\begin{figure*}[tb]
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ev_ping_rule}
    \caption{\textsf{[N]} Varying $\epsilon$}
	  \label{fig:ping-rule}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ev_ping_rule2}
    \caption{\textsf{[N]} Fixed $\epsilon$}
	\label{fig:ping-rule2}
  \end{subfigure}

  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \vspace{12pt} % space of this row from above captions
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ev_chl_rule}
    \caption{\textsf{[C]} Varying $\epsilon$}
	  \label{fig:chl-rule}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ev_chl_rule2}
    \caption{\textsf{[C]} Fixed $\epsilon$}
	\label{fig:chl-rule2}
  \end{subfigure}
  
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \vspace{12pt} % space of this row from above captions
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ev_bgape_rule}
    \caption{\textsf{[A]} Varying $\epsilon$}
	  \label{fig:bgape-rule}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ev_bgape_rule2}
    \caption{\textsf{[A]} Fixed $\epsilon$}
	\label{fig:bgape-rule2}
  \end{subfigure}

  \caption{Performance of different \acl selection rules on the three datasets.
           \textbf{(a), (c), (e)} For varying $\epsilon$ there is no notable difference
           between the three selection rules.
           \textbf{(b), (d), (f)} When using a small fixed value of $\epsilon$ and
           evaluating during each \acl execution, the maximum ambiguity
           selection rule has an advantage over the other two rules.}
  \label{fig:exp}
\end{figure*}

%\setlength\figureheight{1.3in}\setlength\figurewidth{2.1in}
%\input{figures/ev_ping_batch.tex}
%\input{figures/ev_chl_batch.tex}
%\input{figures/ev_bgape_batch.tex}
%\input{figures/limno_bgape_rstr_class400.tex}
\renewcommand\trimlen{2pt}
\begin{figure*}[tb]
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ev_ping_batch}
    \caption{\textsf{[N]} Mean performance}
	  \label{fig:ping-batch}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ev_chl_batch}
    \caption{\textsf{[C]} Mean performance}
	\label{fig:chl-batch}
  \end{subfigure}

  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \vspace{12pt} % space of this row from above captions
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ev_bgape_batch}
    \caption{\textsf{[A]} Mean performance}
	  \label{fig:bgape-batch}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/limno_bgape_rstr_class400}
    \caption{\textsf{[C]} \rstr after $t=400$ iterations}
	\label{fig:bgape-rstr-class}
  \end{subfigure}

  \caption{Performance of explicit threshold batch algorithms on the
           three datasets.
           \textbf{(a)--(c)} \bacl and \bstr achieve comparable performance,
           which is slightly worse than that of their sequential counterparts,
           but significantly better than that of the naive \rstr algorithm.
           \textbf{(d)} The redundant sampling behavior of \rstr resulting
           from not using up-to-date variance estimates.
           }
  \label{fig:exp}
\end{figure*}

\section{Results I:\hspace{0.33em}Explicit threshold level}

\paragraph{Sequential sampling}
\figsref{fig:ping-seq}--\ref{fig:bgape-seq} compare the performance of the
strictly sequential algorithms on the three datasets.
On the first two datasets \acl and \str are comparable in performance, with
\acl being slightly better on the chorophyll dataset and slightly worse
on the latency dataset. However, the situation is different in the third
dataset, where \str performs extremely poorly. The reason is that the
algorithm can get stuck in situations where it is favorable according to
the straddle score to continue sampling points near the currently inferred
level set, instead of sampling points of high variance at yet unexplored
regions that lie far away from the currently inferred level set.

\figref{fig:bgape-str-class} shows an example of such an execution, where
\str has heavily sampled the left region of the level set, but has
completely missed the part of the level set that lies on the right of the
transect (cf. \figref{fig:limno_bgape}).
In \figref{fig:bgape-seq-str-sc} we depict as a scatter plot the
detailed results of the $50$ \str executions. Note that about a third
of the executions do not manage to achieve an $F_1$-score of 0.4, even
after $400$ iterations, i.e. they miss the right part of the level set
as explained above, and another third only achieve an $F_1$-score of 0.8,
i.e. they miss the left part of the level set.
On the other hand, as shown in \figref{fig:bgape-seq-acl-sc}, this problem
never occurs in \acl, because of its classification regime, which implicitly
forces exploration by excluding points that have already been classified from
being sampled.

Although \var is commonly used for estimating functions over their entire
domain, it is clearly outperformed in our experiments, because it does not
sufficiently focus on accurately estimating the desired level set, but
rather samples almost uniformly over the sample space.

\paragraph{Sample selection rules}
In \figsref{fig:ping-rule}, \ref{fig:chl-rule}, and \ref{fig:bgape-rule}
we compare the performance
of \acl using different sample selection rules (\lineref{lin:sel1} of
\algoref{alg:acl}). In particular, we compare the maximum ambiguity
rule (as presented in \acl), the maximum variance rule
(which we use in \iacl), and the rule of randomly choosing the next
sample from $U_t$.

To further investigate the potential effect of the sampling rules, we also
ran a different set of experiments, where, instead of varying $\epsilon$
and evaluating the $F_1$-score after termination, we fixed $\epsilon$ to a
small value ($h/50$) and
evaluated the different \acl variants during their execution using the
GP posterior mean to classify points into the super- and sublevel sets
(as explained before for \str and \var). In other words, we evaluated the
performance obtained when the tuning of $\epsilon$ for controlling the
required number of samples is ignored.

The results are presented in \figsref{fig:ping-rule2}, \ref{fig:chl-rule2},
and \ref{fig:bgape-rule2}. In this case, the maximum ambiguity rule
outperforms the other two rules on all three datasets. However, note
that in the two environmental monitoring datasets the difference is fairly
small and the other two rules still perform considerably better than \var.

To connect the above observations with the operation of \acl, recall
that the algorithm is largely based on the progressive classification of
points into $H_t$ and $L_t$, a process that focuses the selection of the next
sample at each step on a set of ``interesting'' (w.r.t. to the sought after
level set), yet unclassified points $U_t$. The
way in which the sample is selected from $U_t$ seems to be of secondary
importance and selecting using maximum ambiguity might provide a small
performance benefit.

\paragraph{Batch sampling}
In \figsref{fig:ping-batch}--\ref{fig:bgape-batch} we show the performance
of the explicit threshold batch algorithms on the three datasets. The \bacl
and \bstr algorithms,
which use the always up-to-date variance estimates for selecting batches,
achieve similar performance. Furthermore, there is only a slight performance
penalty when compared to the strictly sequential \str, which can easily be
outweighed by the benefits of batch point selection (e.g. in the network
latency application, the batch algorithms would have about $B = 30$ times
higher throughput). On the other hand, as expected, the \rstr algorithm
performs significantly worse than the other two batch algorithms, since it
selects a lot of redundant samples in areas of high straddle score, as depicted
in \figref{fig:bgape-rstr-class} (cf. \sectref{sect:bacl}).

%\setlength\figureheight{1.3in}\setlength\figurewidth{2.1in}
%\input{figures/ev_chl_imp.tex}
%\input{figures/ev_bgape_imp.tex}
%\input{figures/ev_bgape_imp_istr_sc.tex}
%\input{figures/ev_bgape_imp_iacl_sc.tex}
%\input{figures/ev_bgape_imp_istr_h.tex}
%\input{figures/ev_bgape_imp_iacl_h.tex}
\renewcommand\trimlen{2pt}
\begin{figure*}[tb]
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ev_chl_imp}
    \caption{\textsf{[C]} Mean performance}
	  \label{fig:chl-imp}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ev_bgape_imp}
    \caption{\textsf{[A]} Mean performance}
	\label{fig:bgape-imp}
  \end{subfigure}

  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \vspace{12pt} % space of this row from above captions
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ev_bgape_imp_istr_sc}
    \caption{\textsf{[A]} \istr scatter}
	  \label{fig:bgape-imp-istr-sc}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ev_bgape_imp_iacl_sc}
    \caption{\textsf{[A]} \iacl scatter}
	\label{fig:bgape-imp-iacl-sc}
  \end{subfigure}

  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \vspace{12pt} % space of this row from above captions
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ev_bgape_imp_istr_h}
    \caption{\textsf{[A]} \istr threshold level estimate}
	  \label{fig:bgape-imp-istr-h}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ev_bgape_imp_iacl_h}
    \caption{\textsf{[A]} \iacl threshold level estimates}
	\label{fig:bgape-imp-iacl-h}
  \end{subfigure}

  \caption{Performance of implicit threshold algorithms on the environmental
           monitoring datasets.
           \textbf{(a), (b)} \iacl and \ibacl perform somewhat worse than their
           explicit threshold counterparts, while \istr performs notably worse.
           \textbf{(c)} Most of \istr executions fail to achieve high
           $F_1$-scores and top off at about $0.8$.
           \textbf{(d)} \iacl always achieves an $F_1$-score of at least $0.9$
           after $400$ iterations.
           \textbf{(e)} \istr's implicit threshold level estimate is lower than
           the true level and becomes worse over time.
           \textbf{(f)} \iacl's implicit threshold level estimates correctly bound
           the true level and converge towards it over time.
           }
  \label{fig:exp}
\end{figure*}

\section{Results II:\hspace{0.33em}Implicit threshold level}
In \figsref{fig:chl-imp} and~\ref{fig:bgape-imp} we present the results of
executing the implicit threshold algorithms on the two
environmental monitoring datasets. The difficulty of estimating the
function maximum at the same time as performing classification with respect to
the implicit threshold level is manifested in the notably larger sampling cost
of \iacl required to achieve high accuracy compared to the explicit threshold
experiments. As before, the batch version of
\iacl is only slightly worse that its sequential counterpart.

More importantly, the naive \istr algorithm
fails to achieve high accuracy, as it mostly infers wrong estimates of
the function maximum and never recovers, since the (modified) straddle rule
gives no incentive for sampling near the maximum.
This is not very clear in the case of the chlorophyll dataset, because
the function is fairly smooth in the vicinity of its maxima and, thus,
\istr is able to obtain a rather accurate estimate of the function
maximum.
However, in the case of the algae dataset,
as can be seen in \figref{fig:bgape-imp-istr-sc}, most
of the executions of \istr achieve an $F_1$-score of only about
0.8, even after $400$ samples. The inability of \istr to recover from a wrong
threshold estimate is illustrated by the fact that most of these executions
have already achieved this $F_1$-score after less than $100$ samples, but show
no tendency for improvement thereafter. The behavior of \iacl depicted in
more detail in \figref{fig:bgape-imp-istr-sc} is very different
in that it starts considerably slower, but always achieves $F_1$-scores of
at least $0.9$ after $400$ samples.

\figref{fig:bgape-imp-istr-h} shows the estimated implicit threshold
level for an example execution of \istr on the algae concentration
dataset. Note that the inferred level
is smaller than the true level $h = 7$ and, even worse, seems to
deviate further away from the true level over time.
In contrast, as shown in \figref{fig:bgape-imp-iacl-h}, the optimistic
and pessimistic estimates used by \iacl correctly bound the true level
from above and below respectively and get more accurate over time
(as predicted by theory; cf. \lemmaref{lem:hdif}
and~\lemmasref{lem:hopt}--\ref{lem:hpes}),
thus allowing for better classification results.

Our \iacl simulations have shown that, as one might expect, the sooner
the maximum of the function is accurately estimated the faster the
algorithm converges to an accurate classification. Some preliminary
experimentation with simple heuristics, such as sampling according
to the \gpucb rule every $k_t$ steps (with $k_t$ starting small and
increasing with time), leads to improved overall sampling costs and
would be an interesting direction to explore theoretically.

%\section{Results III:\hspace{0.33em}Path planning}
%Finally, \figref{fig:chl-tsp} displays the dramatically lower required travel
%length by using batches of samples for path planning: to achieve
%an $F_1$-score of 0.95 using sequential sampling requires more than six times
%larger traveling distance than planning ahead with $B = 30$
%samples per batch.
