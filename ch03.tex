\chapter{Extensions} \label{sect:ext} 
We now extend \acl to deal with the two problem variants introduced
in \sectref{sect:prelim}, namely implicitly defined threshold levels and
batch point selection. We highlight the key differences
in the extended versions of the algorithm and the resulting implications about
the convergence bound of \theoremref{thm:acl}.

\section{Implicit threshold level} \label{sect:iacl}
The substitution of the explicit threshold level by an implicit level
${h = \omega \max_{\*x\in D} f(\*x)}$ requires modifying the
classification rules as well as the selection rule of \acl, which results
in what we call the \iacl algorithm.

Since $h$ is now an estimated quantity that depends
on the function maximum, we have to take into account the
uncertainty associated with it when making classification decisions.
Concretely, we can obtain an \emph{optimistic estimate} of the function
maximum as ${f^{opt}_{t} = \max_{\*x\in U_t} \max(C_t(\*x))}$ and, analogously,
a \emph{pessimistic estimate} as
${f^{pes}_{t} = \max_{\*x\in U_t} \min(C_t(\*x))}$. The
corresponding estimates of the implicit level are defined as
${h^{opt}_t = \omega f^{opt}_t}$ and ${h^{pes}_t = \omega f^{pes}_t}$, and
can be used in a similar classification scheme to that of \acl.
However, for the above estimates to be correct, we have to ensure that $U_t$
always contains all points that could be maximizers of $f$,
i.e. all points that satisfy $\max(C_t(\*x)) \geq f^{pes}_{t}$.
For that purpose, points that should be classified, but are still possible
function maximizers according to the above inequality, are kept in two sets
$M^H_t$ and $M^L_t$ respectively, while a new set
$Z_t = U_t \cup M^H_t \cup M^L_t$ is used in place of $U_t$ to
obtain the optimistic and pessimistic estimates $h^{opt}_t$
and $h^{pes}_t$.
The resulting classification rules are shown in
\linesref{ln:iclass1}{ln:iclass2} of
\algoref{alg:iacl}, where the conditions are,
again, relaxed by an accuracy parameter $\epsilon$.

\begin{algorithm}[tb]
\caption{The \iacl extension}
\label{alg:iacl}
\small{
\begin{algorithmic}[1]
  \REQUIRE sample set $D$, GP prior ($\mu_0 = 0$, $k$, $\sigma_0$),\\
           \hspace{1.35em}threshold ratio $\omega$, accuracy parameter $\epsilon$
  \ENSURE predicted sets $\hat{H}$, $\hat{L}$
  \STATE $H_0 \gets \varnothing$,\enskip $L_0 \gets \varnothing$,\enskip $U_0 \gets D$\new{,\enskip $Z_0 \gets D$}
  %\LET{$H_0$}{$\varnothing$} \label{lin:init1}
  %\LET{$L_0$}{$\varnothing$}
  %\LET{$U_0$}{$D$}
  %\LET{$Z_0$}{$D$}
  \LET{$C_0(\*x)$}{$\mathbb{R}$, for all $\*x \in D$} \label{lin:init2}
  \LET{$t$}{1}
  \WHILE{$U_{t-1} \neq \varnothing$}
    \STATE $H_t \gets H_{t-1}$,\enskip $L_t \gets L_{t-1}$,\enskip $U_t \gets U_{t-1}$\new{,\enskip $Z_t \gets Z_{t-1}$}
    %\LET{$H_t$}{$H_{t-1}$}
    %\LET{$L_t$}{$L_{t-1}$}
    %\LET{$U_t$}{$U_{t-1}$}
    %\LET{$Z_t$}{$Z_{t-1}$}
    \FORALL{$\*x \in U_{t-1}$}
      \LET{$C_{t}(\*x)$}{$C_{t-1}(\*x) \cap Q_t(\*x)$}
      \new{
      \LET{$h^{opt}_{t}$}{$\omega\max_{\*x\in Z_{t-1}}\max(C_t(\*x))$}
      \LET{$f^{pes}_{t}$}{$\max_{\*x\in Z_{t-1}}\min(C_t(\*x))$}
      \LET{$h^{pes}_{t}$}{$\omega f_{pes}$}
      }
      \IF{$\min(C_t(\*x)) + \epsilon \geq h^{\new{opt}}_{\new{t}}$} \label{ln:iclass1}
        \LET{$U_t$}{$U_t \setminus \{\*x\}$}
        \new{
        \IF{$\max(C_t(\*x)) < f^{pes}_{t}$}
        \LET{$H_t$}{$H_t \cup \{\*x\}$}
        \ELSE
        \LET{$M^H_t$}{$M^H_t \cup \{\*x\}$}
        \ENDIF
        }
      \ELSIF{$\max(C_t(\*x)) - \epsilon \leq h^{\new{pes}}_{\new{t}}$}
        \LET{$U_t$}{$U_t \setminus \{\*x\}$}
        \new{
        \IF{$\max(C_t(\*x)) < f^{pes}_{t}$}
        \LET{$L_t$}{$L_t \cup \{\*x\}$}
        \ELSE
        \LET{$M^L_t$}{$M^L_t \cup \{\*x\}$}
        \ENDIF
        }
      \ENDIF \label{ln:iclass2}
    \ENDFOR
    \new{
    \LET{$Z_t$}{$U_t \cup M^H_t \cup M^L_t$}
    }
    \LET{$\*x_t$}{$\argmax_{\*x \in \new{Z_t}}(\new{w_t}(\*x))$}
    \LET{$y_t$}{$f(\*x_t) + \nu_t$} \label{lin:sel2}
    \STATE Compute $\mu_t(\*x)$ and $\sigma_t(\*x)$, for all $\*x \in U_t$ \label{lin:inf}
    \LET{$t$}{$t + 1$}
  \ENDWHILE
  \LET{$\hat{H}$}{$H_{t-1} \new{\ \cup\ M^H_{t-1}}$}
  \LET{$\hat{L}$}{$L_{t-1} \new{\ \cup\ M^L_{t-1}}$}
\end{algorithmic}
}
\end{algorithm}

In contrast to \acl, which solely focuses on sampling the most ambiguous
points, in \iacl it is also of importance to have a more exploratory
sampling policy in order to obtain more accurate estimates
$f^{opt}_{t}$ and $f^{pes}_{t}$. To this end, we select at each
iteration the point with the largest confidence region \emph{width},
defined as
\begin{align*}
w_t(\*x) = \max(C_t(\*x)) - \min(C_t(\*x)).
\end{align*}
Note that, if confidence intervals were not intersected, this would be
equivalent to maximum variance sampling (within $Z_t$).

%\setlength\figureheight{1.5in}\setlength\figurewidth{2.5in}
%\input{figures/ch03/limno_bgape_imp_class50.tex}
%\input{figures/ch03/limno_bgape_imp_class100.tex}
%\input{figures/ch03/limno_bgape_imp_class200.tex}
%\input{figures/ch03/limno_bgape_imp_class440.tex}
\renewcommand\trimlen{3pt}
\begin{figure}[tb]
  \begin{subfigure}[b]{0.49\linewidth}
    \centering
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ch03/limno_bgape_imp_class50}
    \caption{$t = 50$}
    \label{fig:limno_bgape_imp_class1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\linewidth}
    \centering
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ch03/limno_bgape_imp_class100}
    \caption{$t = 100$}
    \label{fig:limno_bgape_imp_class2}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\linewidth}
    \centering
    \vspace{12pt} % space of this row from above captions
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ch03/limno_bgape_imp_class200}
    \caption{$t = 200$}
    \label{fig:limno_bgape_imp_class3}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\linewidth}
    \centering
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ch03/limno_bgape_imp_class440}
    \caption{$t = 440$ (termination)}
    \label{fig:limno_bgape_imp_class4}
  \end{subfigure}
  \caption{
      Running \iacl with $\epsilon = 0.7$ on a regular grid of $100\times 100$ points
      sampled from the inferred algae concentration GP of \figref{fig:limno_bgape}.
      Regions of already classified points are shown in orange ($H_t$) and blue ($L_t$),
      regions of yet unclassified points ($U_t$) in black, regions of points that could
      be maximizers ($M_t = M_t^L\cup M_t^H$) in green, and observed points
      ($\{\*x_i\}_{1\leq i\leq t}$) as white marks.
  }
\end{figure}

\figsref{fig:limno_bgape_imp_class1} to \ref{fig:limno_bgape_imp_class4}
present an example of running the \iacl algorithm on a fine grid of points
sampled from the inferred GP of the algae concentration dataset shown in
\figref{fig:limno_bgape}. Note how, in contrast to the explicit threshold case,
the sampling process now also focuses on the regions of near-maximal function
value in addition to the ambiguous regions around the implied level set.

\paragraph{Theoretical analysis}
The idea of using the maximum information gain can again be used to provide
a convergence bound for the \iacl algorithm. Some modifications to the analysis
are required due to the difference in the classification rules, i.e. the use
of estimates $h_t^{opt}$ and $h_t^{pes}$ instead of $h$ and the use of the
extended set $Z_t$ instead of $U_t$.
The following theorem expresses for the \iacl algorithm a similar in form bound
to that of \theoremref{thm:acl}.

\begin{theorem}
\label{thm:iacl}
For any $\omega \in (0, 1)$, $\delta \in (0, 1)$, and $\epsilon > 0$,
if $\beta_t = 2\log(|D|\pi^2 t^2/(6\delta))$, \iacl terminates after
at most $T$ iterations, where $T$ is the smallest positive integer
satisfying
\begin{align*}
\frac{T}{\beta_T \gamma_T} \geq \frac{C_1(1+\omega)^2}{4\epsilon^2},
%T/(\beta_T \gamma_T) \geq C_1(1+\omega)^2/(4\epsilon^2),
\end{align*}
where $C_1 = 8/\log(1 + \sigma^{-2})$.

Furthermore, with probability at least $1-\delta$, the algorithm returns
an $\epsilon$-accurate solution with respect to the implicit level
$h = \omega \max_{\*x\in D} f(\*x)$, that is
\begin{align*}
\Pr\left\{\max_{\*x\in D}\ell_h(\*x) \leq \epsilon\right\} \geq 1 - \delta.
\end{align*}
\end{theorem}

The detailed proof of \theoremref{thm:iacl} can be found in
\sectref{sect:app_iacl}. Here we outline the main similarities and differences
compared to the proof of \theoremref{thm:acl}.
As in the explicit threshold case, using the maximum information gain $\gamma_t$,
we show that $w_t(\*x_t)$ decreases as
$\mathcal{O}((\frac{\beta_t \gamma_t}{t})^\frac{1}{2})$\footnote{In fact, we could
have used \iacl's selection rule in \acl without any change in the convergence bound
of \theoremref{thm:acl}. However, as we will see in the following chapter, the
ambiguity-based selection rule performs slightly better in practice.}. Furthermore,
as before, the ``validity'' of the confidence regions is guaranteed by choosing
$\beta_t$ appropriately. However, the steps of proving
termination and $\epsilon$-accuracy need to be modified as follows.
\begin{description}
\item[Termination.] Due to \iacl's classification rules, in order to prove that
      the algorithm terminates (\lemmaref{lem:isterm}), we need to show that
      the gap between the
      optimistic $h_t^{opt}$ and pessimistic $h_t^{pes}$ threshold level
      estimates gets
      small enough. This is accomplished by bounding $h_t^{opt} - h_t^{pes}$
      by a constant multiple of $w_t(\*x_t)$ (\lemmaref{lem:hdif}) and using
      the known decreasing bound on the latter (\lemmaref{lem:iwbound}).
\item[Solution accuracy.] To prove $\epsilon$-accuracy of the returned
      solution with respect to the implicit threshold level, we prove 
      that our optimistic and pessimistic estimates are valid upper and
      lower bounds of the true implicit level at each iteration, i.e.
      that $h_t^{opt} \geq h$ and $h_t^{pes} \leq h$, for all $t\geq 1$
      (\lemmasref{lem:hopt} and~\ref{lem:hpes}). This is the point
      where keeping all possible maximizers as sampling candidates in \iacl
      is formally required
      (cf. \lemmasref{lem:fpes_inc} and \lemmaref{lem:mmeq}).
\end{description}

Note that the sample complexity bound of \theoremref{thm:iacl} is a factor
$(1+\omega)^2\leq 4$ larger than that of \theoremref{thm:acl}, and that
$\omega=0$ actually reduces to an explicit threshold of $0$.

\section{Batch sample selection} \label{sect:bacl}
In the batch setting, the algorithms are only allowed to use the
observed values of previous batches when selecting samples for the
current batch.
A naive way of extending \acl (resp. \iacl) to this setting would be to
modify the selection rule so that, instead of picking the point with
the largest ambiguity (resp.~width), it chooses the $B$ highest ranked
points. However, this approach tends to select ``clusters'' of closely
located samples with high ambiguities (resp.~widths), ignoring
the decrease in the estimated variance of a point resulting from sampling
another point nearby.

Fortunately, we can handle the above issue by exploiting a key property of GPs,
namely that
the predictive variances \eqtref{eq:var} depend only on the selected points
$\*x_t$ and not on the observed values $y_t$ at those points. Therefore,
even if we do not
have available feedback for each selected point up to iteration $t$, we
can still obtain the following useful confidence intervals
\begin{align*}
Q_t^b(\*x) = \left[\mu_{\fb[t]}(\*x) \pm \eta_t^{1/2}\sigma_{t-1}(\*x)\right],
\end{align*}
which combine the most recent available mean estimate
with the always up-to-date variance estimate.
Here $\fb[t]$ is the index of the last available observation expressed
using the feedback function $\fb$ introduced in \sectref{sect:prelim}.
Confidence regions $C_t^b(\*x)$ are
defined as before by intersecting successive confidence intervals and are
used without any further changes in the algorithms.

The pseudocode of \algoref{alg:bacl} highlights the way in which
evaluation feedback is obtained in \bacl. Variable $t_{\fb}$ holds the
latest step for which there is available feedback at each iteration and
the inferred mean is updated whenever new feedback is available, as
dictated by $\fb[t+1]$. However, note that the inferred variance is
updated at each iteration, irrespectively of available feedback.
The batch extension of \iacl works in a completely analogous way.

\begin{algorithm}[tb]
  \caption{The \bacl extension}
  \label{alg:bacl}
\begin{algorithmic}[1]
  \REQUIRE sample set $D$, GP prior ($\mu_0 = 0$, $k$, $\sigma_0$),\\
           \hspace{1.6em}threshold value $h$, accuracy parameter $\epsilon$
  \ENSURE predicted sets $\hat{H}$, $\hat{L}$
  \STATE $H_0 \gets \varnothing$,\enskip $L_0 \gets \varnothing$,\enskip $U_0 \gets D$ \label{lin:binit1}
  %\LET{$H_0$}{$\varnothing$} \label{lin:init1}
  %\LET{$L_0$}{$\varnothing$}
  %\LET{$U_0$}{$D$}
  \LET{$C^{\new{b}}_0(\*x)$}{$\mathbb{R}$, for all $\*x \in D$} \label{lin:binit2}
  \LET{$t$}{1}
  \new{\LET{$t_{\fb}$}{0}}
  \WHILE{$U_{t-1} \neq \varnothing$}
    \STATE $H_t \gets H_{t-1}$,\enskip $L_t \gets L_{t-1}$,\enskip $U_t \gets U_{t-1}$
    %\LET{$H_t$}{$H_{t-1}$}
    %\LET{$L_t$}{$L_{t-1}$}
    %\LET{$U_t$}{$U_{t-1}$}
    \FORALL{$\*x \in U_{t-1}$}
      \LET{$C^{\new{b}}_{t}(\*x)$}{$C^{\new{b}}_{t-1}(\*x) \cap Q^{\new{b}}_t(\*x)$} \label{lin:bupd}
      \IF{$\min(C^{\new{b}}_t(\*x)) + \epsilon > h$} \label{lin:bclass1}
        \LET{$U_t$}{$U_t \setminus \{\*x\}$}
        \LET{$H_t$}{$H_t \cup \{\*x\}$} 
      \ELSIF{$\max(C^{\new{b}}_t(\*x)) - \epsilon \leq h$} \label{lin:bclassr2}
        \LET{$U_t$}{$U_t \setminus \{\*x\}$}
        \LET{$L_t$}{$L_t \cup \{\*x\}$}
      \ENDIF \label{lin:bclass2}
    \ENDFOR
    \LET{$\*x_t$}{$\argmax_{\*x \in U_t}(a^{\new{b}}_t(\*x))$} \label{lin:sel1}
    \new{
    \IF{$\fb[t+1] > t_{\fb}$}
      \FOR{$i = t_{\fb}+1,\ldots,\fb[t+1]$} \label{lin:bsample1}
        \LET{$y_i$}{$f(\*x_i) + \nu_i$}
      \ENDFOR \label{lin:bsample2}
      \STATE Compute $\mu_t(\*x)$ for all $\*x \in U_t$
      \LET{$t_{\fb}$}{$\fb[t+1]$}
    \ENDIF
    }
    \STATE Compute $\sigma_t(\*x)$ for all $\*x \in U_t$
    \LET{$t$}{$t + 1$}
  \ENDWHILE
  \LET{$\hat{H}$}{$H_{t-1}$} \label{lin:bret1}
  \LET{$\hat{L}$}{$L_{t-1}$} \label{lin:bret2}
\end{algorithmic}
\end{algorithm}

\paragraph{Theoretical analysis}
Intuitively, to extend the convergence guarantees of the sequential algorithms
we have to compensate for using outdated mean estimates
by employing a more conservative
(i.e. larger) scaling parameter $\eta_t$ as compared to $\beta_t$.
This ensures that the resulting confidence regions $C_t^b(\*x)$
still contain $f(\*x)$ with high probability.

To appropriately adjust the confidence interval scaling parameter,
in their analysis for extending the \gpucb algorithm to the batch
setting \citet{desautels12} utilized the
\emph{conditional information gain}
\begin{align*}
I(\*y_A; f \mid \*y_{1:\fb[t]}) = H(\*y_A\mid\*y_{1:\fb[t]}) - H(\*y_A\mid f),
\end{align*}
which quantifies the reduction in uncertainty about $f$ by obtaining
a number of observations $\*y_A$, given that we already have observations
$\*y_{1:\fb[t]}$ available.
Following a similar treatment, we extend the convergence bound of
\theoremref{thm:acl} to the batch selection setting of
\bacl via bounding the maximum conditional information gain, resulting
in the following theorem.

\begin{theorem}
\label{thm:bacl}
Assume that the feedback delay $t - \fb[t]$ is at most $B$ for all $t \geq 1$,
where $B$ is a known constant.
Also, assume that for all $t \geq 1$ the maximum conditional mutual information
acquired by any set of measurements since the last feedback is bounded
by a constant $C \geq 0$, i.e.
\begin{align*}
\max_{A\subseteq D, |A|\leq B-1} I(f; \*y_A \mid \*y_{1:\fb[t]}) \leq C
\end{align*}
Then, for any $h\in\mathbb{R}$, $\delta \in (0, 1)$, and $\epsilon \geq 0$,
if $\eta_t = e^C\beta_{fb[t]+1}$, \bacl terminates after
at most $T$ iterations, where $T$ is the smallest positive integer
satisfying
\begin{align*}
\frac{T}{\eta_T \gamma_T} \geq \frac{C_1}{4\epsilon^2},
\end{align*}
where $C_1 = 8/\log(1 + \sigma^{-2})$.

Furthermore, with probability at least $1-\delta$, the algorithm returns
an $\epsilon$-accurate solution, that is
\begin{align*}
\Pr\left\{\max_{\*x\in D}\ell_h(\*x) \leq \epsilon\right\} \geq 1 - \delta.
\end{align*}
\end{theorem}

The detailed proof of \theoremref{thm:bacl} can be found in
\sectref{sect:app_bacl} and a completely analogous theorem can be
formulated for extending \iacl to the batch setting.

Note that, as intuitively described above, the scaling parameter
$\eta_t$ has to increase by a factor of $e^C$ to compensate for the outdated
mean estimates used in the confidence regions $C_t^b(\*x)$. Normally, $C$
depends on the batch size $B$. However, \citet{desautels12} have shown that,
initializing their \gpbucb algorithm with a number of sequentially selected
maximum variance samples, results in a constant factor increase of $\eta_t$
compared to $\beta_t$, independently of $B$.
In the following chapter, we also use maximum variance initialization in our
experiments, which allows us to select a constant value of $\eta_t$
(larger than $\beta_t$)
that works well across different batch sizes.

\section{Path planning} \label{sect:pp} Up to this point, we have assumed that obtaining a sample incurs a fixed
cost independent of its location in the input space $D$. However, in practical
applications like the environmental monitoring example of \chapref{ch:intro},
obtaining a sample also involves moving a mobile sensor to the desired
location, which implies an additional traveling cost that depends on the
measurement location. In what follows, we present two practical ways for
computing appropriate sampling paths: the first is based on the batch sampling
extension of the previous section; the second is tailored to the task of
environmental monitoring in Lake Zurich.

We define a path $\mathcal{P}$ as a vector of points in $D$ and denote by
$\mathcal{P}[i]$ the point that is the $i$-th element of that vector.

\paragraph{Batch-based path planning}
Our first method is a straightforward application of batch point selection for
creating sampling paths. In particular, \bacl is used to select a batch of
$B$ points, but, instead of sampling at those points in the order in which
they were selected (\linesref{lin:bsample1}{lin:bsample2}), we first use a
Euclidean TSP solver to create a path connecting the last sampled point of
the previous batch and all points of the current batch.
\figref{fig:limno_bgape_ppbatch} shows an example of the above procedure
on the algae concentration dataset for a batch size of $B=30$.

%\setlength\figureheight{1.5in}\setlength\figurewidth{2.5in}
%\input{figures/ch03/limno_bgape_ppbatch60_0.tex}
%\input{figures/ch03/limno_bgape_ppbatch60_1.tex}
%\input{figures/ch03/limno_bgape_ppbatch60_2.tex}
%\input{figures/ch03/limno_bgape_ppbatch60_3.tex}
\renewcommand\trimlen{0pt}
\begin{figure}[tb]
  \begin{subfigure}[b]{0.49\linewidth}
    \centering
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ch03/limno_bgape_ppbatch60_0}
    \caption{After previous batch}
    \label{fig:limno_bgape_ppbatch1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\linewidth}
    \centering
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ch03/limno_bgape_ppbatch60_1}
    \caption{Current batch}
    \label{fig:limno_bgape_ppbatch2}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\linewidth}
    \centering
    \vspace{12pt} % space of this row from above captions
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ch03/limno_bgape_ppbatch60_2}
    \caption{TSP path}
    \label{fig:limno_bgape_ppbatch3}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\linewidth}
    \centering
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ch03/limno_bgape_ppbatch60_3}
    \caption{After current batch}
    \label{fig:limno_bgape_ppbatch4}
  \end{subfigure}
  \caption{
      Path planning with \bacl using $\epsilon = 0.7$ on a regular grid of
      $100\times 100$ points sampled from the inferred algae concentration
      GP of \figref{fig:limno_bgape}. Regions of already classified points
      are shown in orange ($H_t$) and blue ($L_t$), regions of yet unclassified
      points ($U_t$) in black, and observed points
      ($\{\*x_i\}_{1\leq i\leq t}$) as white marks.
      \textbf{(a)} The current position, i.e. the last point of the previous
                   batch shown as a white circle.
      \textbf{(b)} The current batch of $B=30$ points shown as red circles.
      \textbf{(c)} TSP path connecting the points of the current batch with
                   the current position using a TSP path.
      \textbf{(d)} The situation after sampling along the TSP path.
  }
  \label{fig:limno_bgape_ppbatch}
\end{figure}

\paragraph{Graph-based path planning}
There are a number of shortcomings when applying the above batch-based method
in practice. First, since selection of the batch is decoupled from its
evaluation, there is no way to adapt the path during sampling of the points
in the TSP path.\footnote{An alternative would be to each time sample the first
point in the TSP path and then replan using a new batch. However, this fails
because it does not give enough incentive to explore areas of the input space
that lie far away from the current position.} Second, additional samples can be obtained while traveling from one location
to another, but there is no easy way to incorporate this when planning using
batches.
Finally, as can be seen in the example of \figref{fig:limno_bgape_ppbatch}
the resulting TSP path may contain several abrupt changes of direction that
are not achievable in practice due to kinematic constraints of the mobile
sensor equipment.

To deal with the above issues, we propose a graph-based algorithm that works
on a more constrained setting, which is suitable for tasks such as the
environmental lake monitoring at hand. In particular, we define a graph with
nodes on a regular grid over the input space and create edges between
adjacent columns of the grid that encode the domain constraints. For example,
in the lake sensing application the robotic vehicle travels at about constant
horizontal speed of $0.7$m/s and the sensor actuator can achieve a maximum
vertical speed of
$0.1$m/s, therefore, if $dx$ and $dy$ are the horizontal and vertical
distances between two nodes, we place an edge between the nodes only if
$dx/dy \geq 7$ (see \figref{fig:graph}).
For simplicity we assume that the sensor can travel across the length
of the transect an indefinite amount of times, but is not allowed to
change its horizontal travel direction unless it reaches the end of the
transect.
Furthermore, we assume that during the traversal of an edge the sensor
obtains $n_e$ measurements equally spaced in time.

We now come to the issue of computing an appropriate path through the graph. 
%\setlength\figureheight{1.5in}\setlength\figurewidth{5.2in}
%\input{figures/ch03/graph.tex} \renewcommand\trimlen{0pt}
\begin{figure}[tb]
  \begin{subfigure}[b]{0.99\linewidth}
    \centering
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ch03/graph}
    \caption{Path planning graph}
    \label{fig:graph}
  \end{subfigure}

  \begin{subfigure}[b]{0.49\linewidth}
    \centering
    \vspace{12pt} % space of this row from above captions
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ch03/limno_bgape_ppbatch60_2}
    \caption{TSP path}
    \label{fig:limno_bgape_ppbatch3}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\linewidth}
    \centering
    \adjincludegraphics[width=\linewidth,clip=true,trim=\trimlen{} \trimlen{} \trimlen{} \trimlen{}]{figures/ch03/limno_bgape_ppbatch60_3}
    \caption{After current batch}
    \label{fig:limno_bgape_ppbatch4}
  \end{subfigure}
  \caption{
      Path planning with \bacl using $\epsilon = 0.7$ on a regular grid of
      $100\times 100$ points sampled from the inferred algae concentration
      GP of \figref{fig:limno_bgape}. Regions of already classified points
      are shown in orange ($H_t$) and blue ($L_t$), regions of yet unclassified
      points ($U_t$) in black, and observed points
      ($\{\*x_i\}_{1\leq i\leq t}$) as white marks.
      \textbf{(a)} The current position, i.e. the last point of the previous
                   batch shown as a white circle.
      \textbf{(b)} The current batch of $B=30$ points shown as red circles.
      \textbf{(c)} TSP path connecting the points of the current batch with
                   the current position using a TSP path.
      \textbf{(d)} The situation after sampling along the TSP path.
  }
  \label{fig:limno_bgape_ppbatch}
\end{figure}